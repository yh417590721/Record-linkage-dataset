{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_1: string (nullable = true)\n",
      " |-- id_2: string (nullable = true)\n",
      " |-- cmp_fname_c1: string (nullable = true)\n",
      " |-- cmp_fname_c2: string (nullable = true)\n",
      " |-- cmp_lname_c1: string (nullable = true)\n",
      " |-- cmp_lname_c2: string (nullable = true)\n",
      " |-- cmp_sex: string (nullable = true)\n",
      " |-- cmp_bd: string (nullable = true)\n",
      " |-- cmp_bm: string (nullable = true)\n",
      " |-- cmp_by: string (nullable = true)\n",
      " |-- cmp_plz: string (nullable = true)\n",
      " |-- is_match: string (nullable = true)\n",
      "\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|     cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "|37291|53113|0.833333333333333|           ?|           1|           ?|      1|     1|     1|     1|      0|    TRUE|\n",
      "|39086|47614|                1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|70031|70237|                1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|84795|97439|                1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|36950|42116|                1|           ?|           1|           1|      1|     1|     1|     1|      1|    TRUE|\n",
      "|42413|48491|                1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|25965|64753|                1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|49451|90407|                1|           ?|           1|           ?|      1|     1|     1|     1|      0|    TRUE|\n",
      "|39932|40902|                1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|46626|47940|                1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"CSCI316-process_three_logisticRegression(simple)\") \\\n",
    ".config(\"spark-master\", \"local\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "df_FD = spark \\\n",
    ".read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\", \"true\").load(\"block_1.csv\")\n",
    "\n",
    "df_FD.printSchema()\n",
    "df_FD.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##following is to do preprocessing\n",
    "from pyspark.sql.functions import when   \n",
    "from pyspark.sql.functions import regexp_replace,col\n",
    "df_FD = df_FD.withColumn('is_match', regexp_replace(col('is_match'), \"FALSE\", \"1\"))\n",
    "df_FD = df_FD.withColumn('is_match', regexp_replace(col('is_match'), \"TRUE\", \"0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['37291',\n",
       " '53113',\n",
       " '0.833333333333333',\n",
       " '?',\n",
       " '1',\n",
       " '?',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert each tuples of RDD to list\n",
    "rdd1 = df_FD.rdd.map(list)\n",
    "rdd1.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.83333333 2.         1.         ... 1.         1.         1.        ]\n",
      " [1.         2.         1.         ... 1.         1.         1.        ]\n",
      " [1.         2.         1.         ... 1.         1.         1.        ]\n",
      " ...\n",
      " [1.         2.         0.09090909 ... 0.         1.         0.        ]\n",
      " [1.         2.         0.11111111 ... 0.         1.         0.        ]\n",
      " [1.         2.         0.         ... 1.         0.         0.        ]]\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#delete first two columns and covert \"?\" to integer 2 ,otherwise float it. \n",
    "def preprocessing(pieces):\n",
    "\n",
    "    scores = [ 2 if p=='?' else float(p) for p in pieces[2:12]]\n",
    "    \n",
    "    return scores\n",
    "\n",
    "dataset = rdd1.map(lambda x: preprocessing(x)).collect()\n",
    "\n",
    "record_linkage = np.array(dataset)\n",
    "\n",
    "X = record_linkage[:, :8]\n",
    "y=record_linkage[:,9]\n",
    "y=y.astype(int)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-348453.66541805235\n",
      "-577.4347979294267\n",
      "-548.714542254037\n",
      "-538.3829217196156\n",
      "-533.4215045056905\n",
      "LOGISTIC REGRESSION FROM SRATCH WEIGHTS =>  [ 19.88446497  -7.81630299   0.40043797 -17.50293336   4.81276592\n",
      "   0.09469743  -3.22698096  -1.53608846  -3.31022303]\n",
      " \n",
      "Confusion matrix:\n",
      "\t [5.72727e+05 9.30000e+01]\n",
      "\t [  24. 2069.]\n",
      "\tAcc:  0.9997964909473259\n",
      "\tPrecision :  0.9999580969740778\n",
      "\tRecall:  0.9998376453336126\n",
      "\tF1-score:  0.9998978675263254\n",
      "-------------------------\n",
      "\n",
      "Accuracy from scratch: 0.9997964909473259\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# GENERATING RANDOM DATA FOR TRAINING TESTING \n",
    "\n",
    "np.random.seed(12)\n",
    "num_observations = 5000\n",
    "\n",
    "\n",
    "simulated_separableish_features = X\n",
    "simulated_labels = y\n",
    "\n",
    "\n",
    "# BUILDING A LINK FUNCTION\n",
    "def sigmoid(scores):\n",
    "    return 1 / (1 + np.exp(-scores))\n",
    "\n",
    "\n",
    "# DEFINING LOG LIKELIHOOD\n",
    "\n",
    "def log_likelihood(features, target, weights):\n",
    "    scores = np.dot(features, weights)\n",
    "    ll = np.sum( target*scores - np.log(1 + np.exp(scores)) )\n",
    "    return ll\n",
    "\n",
    "# BUILDING MAIN LOGISTIC REGRESSION FUNCTION \n",
    "\n",
    "def logistic_regression(features, target, num_steps, learning_rate, add_intercept = False):\n",
    "    if add_intercept:\n",
    "        intercept = np.ones((features.shape[0], 1))\n",
    "        features = np.hstack((intercept, features))\n",
    "        \n",
    "    weights = np.zeros(features.shape[1])\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        scores = np.dot(features, weights)\n",
    "        predictions = sigmoid(scores)\n",
    "\n",
    "        # Update weights with log likelihood gradient\n",
    "        output_error_signal = target - predictions\n",
    "        \n",
    "        gradient = np.dot(features.T, output_error_signal)\n",
    "        weights += learning_rate * gradient\n",
    "\n",
    "        # Print log-likelihood every so often\n",
    "        if step % 10000 == 0:\n",
    "            print (log_likelihood(features, target, weights))\n",
    "        \n",
    "    return weights\n",
    "\n",
    "# WIEGHTS FOR LOGISTIC REGRESSION BUILT FROM SCRATCH\n",
    "\n",
    "weights = logistic_regression(simulated_separableish_features, simulated_labels,\n",
    "                     num_steps = 50000, learning_rate = 5e-5, add_intercept=True)\n",
    "print (\"LOGISTIC REGRESSION FROM SRATCH WEIGHTS => \",weights)\n",
    "\n",
    "\n",
    "final_scores = np.dot(np.hstack((np.ones((simulated_separableish_features.shape[0], 1)),\n",
    "                                 simulated_separableish_features)), weights)\n",
    "\n",
    "preds = np.round(sigmoid(final_scores))\n",
    "\n",
    "label = simulated_labels\n",
    "\n",
    "pred = preds\n",
    "           \n",
    "conf_mat = np.zeros([2, 2])\n",
    "for i in range(len(pred)):\n",
    "\n",
    "    row = int(1 - label[i])\n",
    "    col = int(1 - pred[i])\n",
    "    conf_mat[row][col] += 1\n",
    "\n",
    "TP = conf_mat[0][0]\n",
    "FP = conf_mat[1][0]\n",
    "FN = conf_mat[0][1]\n",
    "TN = conf_mat[1][1]\n",
    "P = conf_mat[0].sum()\n",
    "N = conf_mat[1].sum()\n",
    "All = P + N\n",
    "Precision=TP / (TP + FP)\n",
    "Recall=TP /(TP + FN)\n",
    "    \n",
    "print(\" \")\n",
    "print(\"Confusion matrix:\")\n",
    "print(\"\\t\", conf_mat[0])\n",
    "print(\"\\t\", conf_mat[1])\n",
    "print(\"\\tAcc: \", (TP + TN) / All)\n",
    "print(\"\\tPrecision : \", TP / (TP + FP))\n",
    "print(\"\\tRecall: \",TP /(TP + FN))\n",
    "print(\"\\tF1-score: \",2*(Recall * Precision) / (Recall + Precision))\n",
    "print(\"-------------------------\")\n",
    "print()\n",
    "\n",
    "print ('Accuracy from scratch: {0}'.format((preds == simulated_labels).sum().astype(float) / len(preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

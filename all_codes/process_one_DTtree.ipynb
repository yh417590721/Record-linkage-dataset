{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_1: string (nullable = true)\n",
      " |-- id_2: string (nullable = true)\n",
      " |-- cmp_fname_c1: string (nullable = true)\n",
      " |-- cmp_fname_c2: string (nullable = true)\n",
      " |-- cmp_lname_c1: string (nullable = true)\n",
      " |-- cmp_lname_c2: string (nullable = true)\n",
      " |-- cmp_sex: string (nullable = true)\n",
      " |-- cmp_bd: string (nullable = true)\n",
      " |-- cmp_bm: string (nullable = true)\n",
      " |-- cmp_by: string (nullable = true)\n",
      " |-- cmp_plz: string (nullable = true)\n",
      " |-- is_match: string (nullable = true)\n",
      "\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|     cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "|37291|53113|0.833333333333333|           ?|           1|           ?|      1|     1|     1|     1|      0|    TRUE|\n",
      "|39086|47614|                1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|70031|70237|                1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|84795|97439|                1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|36950|42116|                1|           ?|           1|           1|      1|     1|     1|     1|      1|    TRUE|\n",
      "|42413|48491|                1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|25965|64753|                1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|49451|90407|                1|           ?|           1|           ?|      1|     1|     1|     1|      0|    TRUE|\n",
      "|39932|40902|                1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|46626|47940|                1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"CSCI316-process_one_DTtree(simple)\") \\\n",
    ".config(\"spark-master\", \"local\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "df_FD = spark \\\n",
    ".read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\", \"true\").load(\"block_1.csv\")\n",
    "\n",
    "df_FD.printSchema()\n",
    "df_FD.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##following is to do preprocessing\n",
    "from pyspark.sql.functions import when   \n",
    "from pyspark.sql.functions import regexp_replace,col\n",
    "df_FD = df_FD.withColumn('is_match', regexp_replace(col('is_match'), \"FALSE\", \"1\"))\n",
    "df_FD = df_FD.withColumn('is_match', regexp_replace(col('is_match'), \"TRUE\", \"0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['37291',\n",
       " '53113',\n",
       " '0.833333333333333',\n",
       " '?',\n",
       " '1',\n",
       " '?',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert each tuples of RDD to list\n",
    "rdd1 = df_FD.rdd.map(list)\n",
    "rdd1.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[[0.833333333333333, 2, 1.0, 2, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0], [1.0, 2, 1.0, 2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], [1.0, 2, 1.0, 2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], [1.0, 2, 1.0, 2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], [1.0, 2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], [1.0, 2, 1.0, 2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], [1.0, 2, 1.0, 2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], [1.0, 2, 1.0, 2, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0], [1.0, 2, 1.0, 2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], [1.0, 2, 1.0, 2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "#delete first two columns and covert \"?\" to integer 2 ,otherwise float it. \n",
    "def preprocessing(pieces):\n",
    "    \n",
    "    scores = [ 2 if p=='?' else float(p) for p in pieces[2:12]]\n",
    "    \n",
    "    return scores\n",
    "\n",
    "#convert rdd to list type\n",
    "dataset = rdd1.map(lambda x: preprocessing(x)).collect()\n",
    "\n",
    "#covert to numpy arrray list type\n",
    "record_linkage = np.array(dataset)\n",
    "\n",
    "print(type(record_linkage))\n",
    "print(type(dataset))\n",
    "print(record_linkage[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numer of samples: 574913\n",
      "[  1357. 381918.]\n",
      "<class 'numpy.ndarray'>\n",
      " \n",
      "Confusion matrix:\n",
      "\t [190902.      0.]\n",
      "\t [736.   0.]\n",
      "\tAcc:  0.9961594255836526\n",
      "\tPrecision :  0.9961594255836526\n",
      "\tRecall:  1.0\n",
      "\tF1-score:  0.9980760181941758\n",
      "-------------------------\n",
      "\n",
      "\tAccuracy from scratch:  0.9961594255836526\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "import time\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def calAccuracy(pred, label):\n",
    "    acc = 0.0\n",
    "    for i in range(len(pred)):\n",
    "        if (pred[i] == label[i][-1]):\n",
    "            acc += 1\n",
    "    return acc / len(pred)\n",
    "\n",
    "\n",
    "def classify2(node, dataSet):\n",
    "    if not isinstance(node, dict):\n",
    "        return np.argmax(node)\n",
    "\n",
    "    if (dataSet[node['spInd']] < node['spVal']):\n",
    "        C = classify2(node['left'], dataSet)\n",
    "    else:\n",
    "        C = classify2(node['right'], dataSet)\n",
    "\n",
    "    return C\n",
    "\n",
    "def classify1(node, dataSet):\n",
    "    result = np.zeros(len(dataSet))\n",
    "    for i in range(len(dataSet)):\n",
    "        sample = dataSet[i]\n",
    "        result[i] = classify2(node, sample)\n",
    "\n",
    "    return result\n",
    "\n",
    "def BinarySplit(dataSet, featIndex, splitVal):\n",
    "    subData0 = dataSet[dataSet[:, featIndex] < splitVal]\n",
    "    subData1 = dataSet[dataSet[:, featIndex] >= splitVal]\n",
    "\n",
    "    return subData0, subData1\n",
    "\n",
    "def NewS(subData0, subData1):\n",
    "    if (len(subData0) == 0 or len(subData1) == 0):\n",
    "        return inf\n",
    "\n",
    "    total = len(subData0) + len(subData1)\n",
    "    p0 = len(subData0) / total\n",
    "    p1 = len(subData1) / total\n",
    "\n",
    "    newS = ShannoEnt(subData0) * p0 + ShannoEnt(subData1) * p1\n",
    "\n",
    "    return newS\n",
    "\n",
    "\n",
    "def BaseS(dataSet):\n",
    "    \n",
    "    S = ShannoEnt(dataSet)\n",
    "\n",
    "    return S\n",
    "\n",
    "def ShannoEnt(dataSet):\n",
    "    numOfG = dataSet[:, -1].sum()\n",
    "    pG = numOfG / len(dataSet)\n",
    "    pH = 1 - pG\n",
    "\n",
    "    if (pH == 0 or pG == 0):\n",
    "        return 0\n",
    "    shannonEnt = -pH * np.log2(pH) - pG * np.log2(pG)\n",
    "\n",
    "    return shannonEnt\n",
    "\n",
    "\n",
    "def chooseBestSplit(dataSet,ops=(0.05, 6)):\n",
    "    tolS = ops[0]  #early stoping for information gain but didint implement\n",
    "    tolN = ops[1]  #early stoping for number of coloumn\n",
    "    n = dataSet.shape[1]\n",
    "    S = BaseS(dataSet)  #base entropy\n",
    "    bestS = inf;bestIndex = 0;bestSplitVal = 0\n",
    "    #call for lops to iterate each available split condition,then calculate thier entropy,\n",
    "    #then calculate thier criterionï¼Œsuch asinformaton gain,finally contrast thier criterion to choose best splition\n",
    "    for featIndex in range(n - 1):\n",
    "       # print(\"---------------------------------------------\")\n",
    "       # print(\"featIndex  \"+str(featIndex))\n",
    "        for splitVal in set(dataSet[:, featIndex]):\n",
    "            #print(\"splitVal  \"+str(splitVal))\n",
    "            #call binary split to choose what sub data\n",
    "            subDS0, subDS1 = BinarySplit(dataSet, featIndex, splitVal)\n",
    "           # print(\"subDS0  \"+str(subDS0))\n",
    "            newS = NewS(subDS0, subDS1)\n",
    "           # print(\"newS  \"+str(newS))\n",
    "             #contrast\n",
    "            if (newS < bestS):\n",
    "                bestS = newS\n",
    "                bestIndex = featIndex\n",
    "                bestSplitVal = splitVal\n",
    "\n",
    "    if (S - bestS) < tolS:\n",
    "        return None, 0\n",
    "\n",
    "    # check valid split again and return bestIndex, bestSplitValue\n",
    "\n",
    "    subDS0, subDS1 = BinarySplit(dataSet, bestIndex, bestSplitVal)\n",
    "\n",
    "    if (np.shape(subDS0)[0] < tolN) or (np.shape(subDS1)[0] < tolN):\n",
    "        return None, 0\n",
    "    return bestIndex, bestSplitVal\n",
    "\n",
    "def induction(dataSet):\n",
    "    n = {}\n",
    "    total = dataSet[:, -1].sum()\n",
    "    freqClasses = np.array([len(dataSet) - total, total])\n",
    "    #If it has only one category, the split is stopped\n",
    "    if len(set(dataSet[:, -1])) == 1:\n",
    "         return dataSet[0, -1]\n",
    "    #judge which criteria it belongs to\n",
    "    #the data divided may not all belong to a class, at this time, the classification of the sub-data set needs to be determined according to the majority voting rule.\n",
    "\n",
    "    bestIndex, bestSplitVal = chooseBestSplit(dataSet)\n",
    "    \n",
    "    # no feature ,so stop slit\n",
    "    if (bestIndex == None):\n",
    "        return freqClasses\n",
    "\n",
    "     #call binary split to choose what sub data\n",
    "    subDataLeft, subDataRight = BinarySplit(dataSet, bestIndex, bestSplitVal)\n",
    "\n",
    "    n['spInd'] = bestIndex\n",
    "    n['spVal'] = bestSplitVal\n",
    "\n",
    "    n_chaild_l = induction(subDataLeft)\n",
    "    n['left'] = n_chaild_l\n",
    "\n",
    "    n_chaild_r = induction(subDataRight)\n",
    "    n['right'] = n_chaild_r\n",
    "    \n",
    "    \n",
    "    return n\n",
    "\n",
    "class DecisionTree:\n",
    "    \n",
    "    def __init__(self, train_data):\n",
    "        self.train_data = train_data\n",
    "        self.root = induction(train_data)\n",
    "        print(self.root)\n",
    "        print(type(self.root))\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        return classify1(self.root, test_data)\n",
    "\n",
    "    def calAcc(self, test_data):\n",
    "        predict = self.predict(test_data)\n",
    "        return calAccuracy(predict, test_data)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    dataSet = record_linkage\n",
    "\n",
    "    #here can set the ratio og remaining dataset,because speed of calculating is alright,so I set 1,means keep original dataset------------------------ \n",
    "    ratio=1\n",
    "    \n",
    "    reduced_dataSet_number=int(ratio*len(dataSet))\n",
    "    \n",
    "    print(\"numer of samples: \" +  str(reduced_dataSet_number))\n",
    "    \n",
    "    shuffled_indices=np.random.permutation(reduced_dataSet_number)\n",
    "    \n",
    "    reduced_index=shuffled_indices[:reduced_dataSet_number]\n",
    "    \n",
    "    reduced_data=dataSet[reduced_index, :]\n",
    "    #----------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    l = len(reduced_data)\n",
    "    \n",
    "    indices = np.random.permutation(reduced_data.shape[0])\n",
    "\n",
    "    train_idx, test_idx = indices[:int(2 * l / 3)], indices[int(2 * l / 3):]\n",
    "\n",
    "    train_data, test_data = reduced_data[train_idx, :], reduced_data[test_idx, :]\n",
    "\n",
    "    M_IG = DecisionTree(train_data)\n",
    "\n",
    "    label = test_data[:, -1]\n",
    "\n",
    "    pred = M_IG.predict(test_data)\n",
    "           \n",
    "    conf_mat = np.zeros([2, 2])\n",
    "    for i in range(len(pred)):\n",
    "\n",
    "         row = int(1 - label[i])\n",
    "         col = int(1 - pred[i])\n",
    "         conf_mat[row][col] += 1\n",
    "\n",
    "    TP = conf_mat[0][0]\n",
    "    FP = conf_mat[1][0]\n",
    "    FN = conf_mat[0][1]\n",
    "    TN = conf_mat[1][1]\n",
    "    P = conf_mat[0].sum()\n",
    "    N = conf_mat[1].sum()\n",
    "    All = P + N\n",
    "    Precision=TP / (TP + FP)\n",
    "    Recall=TP /(TP + FN)\n",
    "    \n",
    "    print(\" \")\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(\"\\t\", conf_mat[0])\n",
    "    print(\"\\t\", conf_mat[1])\n",
    "    print(\"\\tAcc: \", (TP + TN) / All)\n",
    "    print(\"\\tPrecision : \", TP / (TP + FP))\n",
    "    print(\"\\tRecall: \",TP /(TP + FN))\n",
    "    print(\"\\tF1-score: \",2*(Recall * Precision) / (Recall + Precision))\n",
    "    print(\"-------------------------\")\n",
    "    print()\n",
    "    print(\"\\tAccuracy from scratch: \",(TP + TN) / All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
